{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravichas/LLM/blob/main/SemanticModeling_GCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Modeling Using GCP Tips\n",
        "\n",
        "Based on DeepLearning.AI short course"
      ],
      "metadata": {
        "id": "hiRsgCi3ZC5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so_questions = so_database.input_text.tolist()\n",
        "question_embeddings = encode_text_to_embedding_batched(sentences = so_questions, api_calls_per_second = 20/60,\n",
        "batch_size = 5)\n",
        "\n",
        "# save the pre-computed embeddings are saved in a pickle file (bucket)\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('question_embeddings_app.pkl', 'rb') as file:\n",
        "  # call load method to deseralize\n",
        "  question_embeddings = pickle.load(file)\n",
        "  print(question_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# How to embed and add new embeddings\n",
        "\n",
        "emb = model.get_embeddings([input_text])[0].values\n",
        "\n",
        "embeddings_l = question_embeddings.tolist()\n",
        "# add the new embedding\n",
        "\n",
        "embeddings_array = np.array(embeddings_l)\n",
        "\n",
        "print(\"Shape: \" + str(embeddings_array.shape))\n",
        "print(embeddings_array)\n",
        "\n",
        "# add the new data to the existing df\n",
        "\n",
        "new_row = pd.Series([input_text, None, \"baking\"], index=so.df.columns)\n",
        "so_df.loc[len(so_df)+1] = new_row\n",
        "\n",
        "clf = IsolationForest(contaimination = 0.005, random_state = 2)\n",
        "\n",
        "# to view the outliers\n",
        "preds = clf.fit_predict(embeddings_array)\n",
        "\n",
        "# drop the newly addded row\n",
        "so_df = so_df.drop(so_df.index[-1])\n",
        "\n",
        "# classification\n",
        "# using emb vectors --> str data\n",
        "# use it for classification\n",
        "\n",
        "\n",
        "X = question_embeddings\n",
        "y = so_df['category'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "y, test_size = 0.2, random_state = 2)\n",
        "\n",
        "clf = RandomForestClassifer(n_estimators = 200)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate Text\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "generation_Model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "prompt = \"Recommend me a programming activity to improve my skills\"\n",
        "\n",
        "generation_model.predict(prompt = prompt)\n",
        "\n",
        "# Decoding Strategies\n",
        "\n",
        "text = \"The garden was full of beautiful\"\n",
        "\n",
        "We can create a probabilities of the next text or tokens (ex [flowers (0.5), trees (0.23), herbs(0.05), ... bugs (0.03)]\n",
        "\n",
        "probabilities --> Decoding strategy\n",
        "\n",
        "* Greedy Decoding; The one with highest prob.\n",
        "* Random sample: Use the probabilities sample a random token\n",
        "* One of the parameters we can use is called temperature (controls the randomness); low temp are good for good prediction and high temp is good for openended conversation\n",
        "\n",
        "In Neural Network we wil have outputs called logits. We pass these logits to Softmax to get probability distribution over classes.\n",
        "\n",
        "Usually (z_i) is the input to softmax. We can divide z_i/$\\theta$ (temperature is $\\theta$) and pass it into softmax\n",
        "\n",
        " ```\n",
        " word  Logits Softmax Softmax with Temperature\n",
        "                         0.1.     higher\n",
        " --------------------------------------------\n",
        " flowers.  20  0.881.   1.0        0.28\n",
        " trees     18  0.119.   0.00       0.20\n",
        " herbs      5  0.000    0.00       0.15\n",
        " bugs       2. 0.000.   0.00       0.10\n",
        " ```\n",
        "\n",
        "prompt = \"I reached into my toolkit to fetch my:\"\n",
        "\n",
        "response = generation_model.predict(prompt = prompt, temperature = temperature, )\n",
        "\n",
        "print(f\"[Temperature = {temperature}]\")\n",
        "print(response.text)\n",
        "\n",
        "\n",
        "Temperature of 0: Deterministic choice; Not creative but reliable\n",
        "\n",
        "Temperature is increased: Randomness is increased; creative but sometimes unreasonable\n",
        "\n",
        "# high temp\n",
        "\n",
        "temperature = 1.0\n",
        "\n",
        "response = generation_model.predict(prompt = prompt, temperature = temperature, )\n",
        "\n",
        "print(f\"[Temperature = {temperature}]\")\n",
        "print(response.text)\n",
        "\n",
        "# top k\n",
        "\n",
        "Another Hyp Param is top-k, this means picking top-k probabilities instead of top-1\n",
        "\n",
        "# top p\n",
        "\n",
        "Sample from min set of tokens whose cumulative prob is >= p\n",
        "\n",
        "ex\n",
        "(ex [flowers (0.5), trees (0.23), herbs(0.05), ... bugs (0.03)]\n",
        "\n",
        "top-p = 0.75; then flowers, trees and herbes will be selected.\n",
        "\n",
        "We dont have to select one strategy\n",
        "\n",
        "# putting it all together\n",
        "\n",
        "* Top K is done first\n",
        "* from the results top-P is filtered\n",
        "* Finally Temperature is applied\n",
        "\n",
        "ex\n",
        "\n",
        "Text: The garden was full of beautiful\n",
        "\n",
        "[flowers, trees,herbs, plants, ..., weeds, bugs]\n",
        "\n",
        "Top K [flowers, trees, hers, plants]\n",
        "Top P [flowers, trees]\n",
        "Temperature [trees]\n",
        "\n",
        "output: trees\n",
        "\n",
        "Example\n",
        "\n",
        "prompt = \"Write an advertisement for jackets that involve blue elephants and avacados\"\n",
        "\n",
        "top_p = 0.7 # 0 to 1\n",
        "top_k = 20  # 0 to 40\n",
        "\n",
        "response = generation_model.predict(prompt = prompt, temperature = 0, )\n",
        ".9, top_k=top_k, top_p = top_p)\n",
        "\n",
        "print(f\"[Temperature = {temperature}]\")\n",
        "print(top_p)\n",
        "print(response.text)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JyoIl8AZiDlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding strategies for GCP"
      ],
      "metadata": {
        "id": "Vj_SCPwVzgL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock overflow questions (so questions)\n",
        "so_questions = so_database.input_text.tolist()\n",
        "question_embeddings = encode_text_to_embedding_batched(sentences = so_questions, api_calls_per_second = 20/60,\n",
        "batch_size = 5)\n",
        "\n",
        "# save the pre-computed embeddings are saved in a pickle file (bucket)\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('question_embeddings_app.pkl', 'rb') as file:\n",
        "  # call load method to deseralize\n",
        "  question_embeddings = pickle.load(file)\n",
        "  print(question_embeddings)\n",
        "\n",
        "# add the embeddings as a column\n",
        "so_database['embeddings'] = question_embeddings.tolist()\n",
        "\n",
        "# explore df\n",
        "so_database.head(5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_avOEn4ViWmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strategy\n",
        "\n",
        "embedded user query --> compute similarity with alll so_questions\n",
        "\n",
        "While computing distances, we need a distance metric, one of the metric that we could use is\n",
        "* L2 distance (Euclidean) distance betwen the ends of the two vectors\n",
        "* Manhattan distance (distance between the ends of two vectors)\n",
        "* cosine similarity (angle)\n",
        "* dot product (angle + magnitude)"
      ],
      "metadata": {
        "id": "I_kJMaIO0j0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to concat dataframes pandas\"\n",
        "query_embedding = embedding_model.get_embeddings(query)[0].values\n",
        "\n",
        "# cosine similarity is a scikit function\n",
        "# we are wrapping the query_embedding as a list '[]' becos\n",
        "# the input is a list of lists\n",
        "# so_database --> array --> converting into a 2d list\n",
        "cos_sim_array= cosine_similarity([query_embedding],\n",
        "                                 list(so_database.embeddings.values))\n",
        "\n",
        "# other metrics\n",
        "# distances_argmin instead of cosine_similarity\n",
        "# index_doc_distances = distances_argmin([query_embedding],\n",
        "                                      #  list(so_database.embeddings.values))[0]\n",
        "\n",
        "cos_sim_array # (1, 2000) 1 row 2000 col becos of 1 query\n",
        "\n",
        "index_doc_cosine = np.argmax(cos_sim_array)\n",
        "\n",
        "so_database.input_text[index_doc_cosine]\n",
        "\n",
        "# let us also look at output\n",
        "so_database.output_text[index_doc_cosine]\n",
        "\n"
      ],
      "metadata": {
        "id": "Yie5o2Cb1av6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output formatting is not user-friendly. To create a better formatting. Let us use LLM to format the data"
      ],
      "metadata": {
        "id": "8QaYqbkp5Do9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Question: \" + so_database.input_text[index_doc_cosine] + \\\n",
        "\"\\n Answer: \" + so_database.output_text[index_doc_cosine]\""
      ],
      "metadata": {
        "id": "Kd0Pi-tC5MOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# How to embed and add new embeddings\n",
        "\n",
        "emb = model.get_embeddings([input_text])[0].values\n",
        "\n",
        "embeddings_l = question_embeddings.tolist()\n",
        "# add the new embedding\n",
        "\n",
        "embeddings_array = np.array(embeddings_l)\n",
        "\n",
        "print(\"Shape: \" + str(embeddings_array.shape))\n",
        "print(embeddings_array)\n",
        "\n",
        "# add the new data to the existing df\n",
        "\n",
        "new_row = pd.Series([input_text, None, \"baking\"], index=so.df.columns)\n",
        "so_df.loc[len(so_df)+1] = new_row"
      ],
      "metadata": {
        "id": "SDTkvhbb0gwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Example"
      ],
      "metadata": {
        "id": "wYLpkeGjzcEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = IsolationForest(contaimination = 0.005, random_state = 2)\n",
        "\n",
        "# to view the outliers\n",
        "preds = clf.fit_predict(embeddings_array)\n",
        "\n",
        "# drop the newly addded row\n",
        "so_df = so_df.drop(so_df.index[-1])\n",
        "\n",
        "# classification\n",
        "# using emb vectors --> str data\n",
        "# use it for classification\n",
        "\n",
        "X = question_embeddings\n",
        "y = so_df['category'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "y, test_size = 0.2, random_state = 2)\n",
        "\n",
        "clf = RandomForestClassifer(n_estimators = 200)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "96008W0KzTP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text Application"
      ],
      "metadata": {
        "id": "P0rOvWiBzX0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Text\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "generation_Model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "prompt = \"Recommend me a programming activity to improve my skills\"\n",
        "\n",
        "generation_model.predict(prompt = prompt)\n",
        "\n",
        "# Decoding Strategies\n",
        "\n",
        "text = \"The garden was full of beautiful\""
      ],
      "metadata": {
        "id": "y6b0hbKezWh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a probabilities of the next text or tokens (ex [flowers (0.5), trees (0.23), herbs(0.05), ... bugs (0.03)]\n",
        "\n",
        "probabilities --> Decoding strategy\n",
        "\n",
        "* Greedy Decoding; The one with highest prob.\n",
        "* Random sample: Use the probabilities sample a random token\n",
        "* One of the parameters we can use is called temperature (controls the randomness); low temp are good for good prediction and high temp is good for openended conversation\n",
        "\n",
        "In Neural Network we wil have outputs called logits. We pass these logits to Softmax to get probability distribution over classes.\n",
        "\n",
        "Usually (z_i) is the input to softmax. We can divide z_i/$\\theta$ (temperature is $\\theta$) and pass it into softmax\n",
        "\n",
        "```\n",
        " word  Logits Softmax Softmax with Temperature\n",
        "                         0.1.     higher\n",
        " --------------------------------------------\n",
        " flowers.  20  0.881.   1.0        0.28\n",
        " trees     18  0.119.   0.00       0.20\n",
        " herbs      5  0.000    0.00       0.15\n",
        " bugs       2. 0.000.   0.00       0.10\n",
        " ```"
      ],
      "metadata": {
        "id": "-dTPuELGxcBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I reached into my toolkit to fetch my:\"\n",
        "\n",
        "response = generation_model.predict(prompt = prompt, temperature = temperature, )\n",
        "\n",
        "print(f\"[Temperature = {temperature}]\")\n",
        "print(response.text)\n",
        "\n",
        "\n",
        "Temperature of 0: Deterministic choice; Not creative but reliable\n",
        "\n",
        "Temperature is increased: Randomness is increased; creative but sometimes unreasonable\n",
        "\n",
        "# high temp\n",
        "\n",
        "temperature = 1.0\n",
        "\n",
        "response = generation_model.predict(prompt = prompt, temperature = temperature, )\n",
        "\n",
        "print(f\"[Temperature = {temperature}]\")\n",
        "print(response.text)\n",
        "\n"
      ],
      "metadata": {
        "id": "9K6S0RzXxaGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different strategies:\n",
        "\n",
        "# top k\n",
        "Another Hyp Param is top-k, this means picking top-k probabilities instead of top-1\n",
        "\n",
        "# top p\n",
        "Sample from min set of tokens whose cumulative prob is >= p\n",
        "\n",
        "## ex\n",
        "(ex [flowers (0.5), trees (0.23), herbs(0.05), ... bugs (0.03)]\n",
        "\n",
        "top-p = 0.75; then flowers, trees and herbes will be selected.\n",
        "\n",
        "We dont have to select one strategy"
      ],
      "metadata": {
        "id": "n9MSvHGby3L_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCGkzm96yxFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all to-gether\n",
        "\n",
        "* Top K is done first\n",
        "* from the results top-P is filtered\n",
        "* Finally Temperature is applied\n",
        "\n",
        "# Example\n",
        "* Input Text: The garden was full of beautiful\n",
        "\n",
        "```[flowers, trees,herbs, plants, ..., weeds, bugs]```\n",
        "\n",
        "Filters applied in the following order:\n",
        "* Top K [flowers, trees, hers, plants]\n",
        "* Top P [flowers, trees]\n",
        "* Temperature [trees]\n",
        "\n",
        "* Output: trees"
      ],
      "metadata": {
        "id": "akV26hkNyJdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "coTGzsaEyFjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an advertisement for jackets that involve blue elephants and avacados\"\n",
        "\n",
        "top_p = 0.7 # 0 to 1\n",
        "top_k = 20  # 0 to 40\n",
        "\n",
        "response = generation_model.predict(prompt = prompt,\n",
        "                                    temperature = 0.9,\n",
        "                                    top_k=top_k,\n",
        "                                    top_p = top_p)\n",
        "\n",
        "print(f\"[Temperature = {temperature}]\")\n",
        "print(top_p)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "D47FqAzlx-pS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}